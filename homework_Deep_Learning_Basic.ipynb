{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64efce43",
   "metadata": {},
   "source": [
    "### Подготовка данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e090bc",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d3ff6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for training\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58226016",
   "metadata": {},
   "source": [
    "Static configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "883d46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_2_SAMPELS = 'samples/*'\n",
    "TRAIN_DIR = 'data/train'\n",
    "TEST_DIR = 'data/test'\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "ALPHABETH = 'abcdefghijklmnopqrstuvwxyz'\n",
    "DIGITS = '0123456789'\n",
    "CHARS = ALPHABETH + DIGITS\n",
    "VOCAB_SIZE = len(CHARS) + 1\n",
    "\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7627cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    class for getting data and labels\n",
    "    \"\"\"\n",
    "    def __init__(self, pth):\n",
    "        \"\"\"\n",
    "        collect data\n",
    "        \"\"\"\n",
    "        pth_list = os.listdir(pth)\n",
    "        abs_pth = os.path.abspath(pth)\n",
    "        print(abs_pth)\n",
    "        self.imgs = [os.path.join(abs_pth, p) for p in pth_list]\n",
    "        self.transform = transforms.Compose([\n",
    "          transforms.ToTensor()  \n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        get length of the data\n",
    "        \"\"\"\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(seld, idx):\n",
    "        \"\"\"\n",
    "        get tensor of the image and label by index\n",
    "        \"\"\"\n",
    "        pth = self.img_list[idx]\n",
    "        label = os.path.basename(pth).split('.')[0].lower().strp()\n",
    "        img = Image.open(pth).convert('RGB')\n",
    "        img_tensor = self.transform(img)\n",
    "        return img_tensor, label\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e442603a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imgs': [],\n",
       " 'transform': Compose(\n",
       "     ToTensor()\n",
       " )}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "eb1d31ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "mkdir: data: File exists\n"
     ]
    }
   ],
   "source": [
    "# prepare folders\n",
    "os.system(f'mkdir data')\n",
    "os.system(f\"rm -rf data/train\")\n",
    "os.system(f\"rm -rf data/test\")\n",
    "[os.system(f'mkdir data/{directory}') for directory in ('train', 'test')]\n",
    "\n",
    "\n",
    "# split data for train and test 1/4\n",
    "os.system(f'mkdir data')\n",
    "os.system(f\"rm -rf data/train\")\n",
    "os.system(f\"rm -rf data/test\")\n",
    "[os.system(f'mkdir data/{directory}') for directory in ('train', 'test')]\n",
    "\n",
    "[os.system(f\"cp {te} {VAL_DIR}/{te.split('/')[1]}\") for te in test_set]\n",
    "[os.system(f\"cp {tr} {TRAIN_DIR}/{tr.split('/')[1]}\") for tr in train_set]\n",
    "\n",
    "train_dataset = CaptchDataset(TRAIN_DIR)\n",
    "val_dataset = CaptchDataset(VAL_DIR)\n",
    "\n",
    "\n",
    "# loading data\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c86ae",
   "metadata": {},
   "source": [
    "### Создание и обучение модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674d7ea",
   "metadata": {},
   "source": [
    "CRNN class with layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ec5a7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, dropout=0.5):\n",
    "        \"\"\"\n",
    "        initiate layers\n",
    "        \"\"\"\n",
    "        super(CNNRNN, self).__init__()\n",
    "        # probability of an element to be zeroed (Bernoulli distribution)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.clayer = nn.Sequential(\n",
    "            # With square kernels and equal stride and with padding\n",
    "            nn.Conv2d(\n",
    "                in_channels=3,\n",
    "                out_channels=32, \n",
    "                kernel_size=(3,3),\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            # activation max(0,x)\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=(2,2),\n",
    "                stride=2\n",
    "            ),\n",
    "            # in_channels, out_channels, kernel_size\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=(3,3),\n",
    "                stride=1, \n",
    "                padding=1),\n",
    "            nn.ReLU(),\n",
    "            # pool of square window of size=2, stride=2\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=(2,2), \n",
    "                stride=2\n",
    "            ),\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=(3,3), \n",
    "                stride=1, \n",
    "                padding=1\n",
    "            ),\n",
    "            nn.ReLU(), # max(0,x)\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                in_channels=128,\n",
    "                out_channels=256, \n",
    "                kernel_size=(3,3), \n",
    "                stride=1, \n",
    "                padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=(1,2), \n",
    "                stride=2\n",
    "            ),\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                in_channels=256,\n",
    "                out_channels=512,\n",
    "                kernel_size=(3,3), \n",
    "                stride=1, \n",
    "                padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                in_channels=512,\n",
    "                out_channels=512,\n",
    "                kernel_size=(3,3), \n",
    "                stride=1, \n",
    "                padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=(1, 2), \n",
    "                stride=2),\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                in_channels=512,\n",
    "                out_channels=512,\n",
    "                kernel_size=(2,2), \n",
    "                stride=1, \n",
    "                padding=1\n",
    "            ),\n",
    "            self.dropout\n",
    "        )\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=1024, \n",
    "                out_features=256\n",
    "            ),\n",
    "            self.dropout\n",
    "        )\n",
    "        # RNN to an input sequence\n",
    "        self.rnn_layer = nn.GRU(\n",
    "            input_size=256,\n",
    "            hidden_size=256, \n",
    "            num_layers=2, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        # Linear transformation to the incoming data\n",
    "        self.outlayer = nn.Linear(\n",
    "            in_features=512, \n",
    "            out_features=vocab_size\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        method for CNN and RNN combining \n",
    "        \"\"\"\n",
    "        x = self.clayer(x) # CNN apply\n",
    "        x = x.permute(0, 3, 1, 2) # transformate dimension\n",
    "        x = x.view(x.size(0), x.size(1), -1) # choose field\n",
    "        x = self.seq(x) # apply linear with dropout\n",
    "        x, _ = self.rnn_layer(x) # apply rnn layers\n",
    "        x = self.outlayer(x) # apply linear layer\n",
    "        return x.permute(1, 0, 2) # transformate dimension\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd34f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "61b13648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCR:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.crnn = CNNRNN(VOCAB_SIZE).to(self.device)\n",
    "        self.critertion = nn.CTCLoss(blank=0)\n",
    "        self.char2idx, self.idx2char = self.char_idx()\n",
    "        \n",
    "        \n",
    "    def char_idx(self):\n",
    "        char2idx = {}\n",
    "        idx2char = {}\n",
    "        chars = CHARS.lower() + '-'\n",
    "        for i, chara in enumerate(chars):\n",
    "            char2idx[chara] = i + 1\n",
    "            idx2char[i + 1] = chara\n",
    "        return char2idx, idx2char\n",
    "    \n",
    "    def encode(self, labels):\n",
    "        length_per_label = [len(label) for label in labels]\n",
    "        joined_label = ''.join(labels)\n",
    "        joined_encoding = []\n",
    "        for chara in joined_label:\n",
    "            joined_encoding.append(self.char2idx[chara])\n",
    "        return torch.IntTensor(joined_encoding), torch.IntTensor(length_per_label)\n",
    "    \n",
    "    def decode(self, logits):\n",
    "        tokens = logits.softmax(2).argmax(2).sqeeze(1)\n",
    "        tokens = ''.join([self.ix2char[token]\n",
    "                         if token != 0 else '-'\n",
    "                         for token in tokens.numpy()])\n",
    "        tokens = tokens.split('-')\n",
    "        \n",
    "        text = [chara for batch_token in tokens\n",
    "               for idx, chara in enumerate(batch_token)\n",
    "               if chara != batch_token[idx-1] or len(barth_token) == 1]\n",
    "        text = ''.join(text)\n",
    "        return text\n",
    "    \n",
    "    def loss_func(self, logits, labels):\n",
    "        encoded_labels, labels_len = self.encode(labels)\n",
    "        logits_lens = torch.full(\n",
    "            size=(logits.size(1),),\n",
    "            fill_value=logits.size(0),\n",
    "            dtype=torch.int32\n",
    "        ).to(self.device)\n",
    "        \n",
    "        return self.critertion(\n",
    "            logits.log_softmax(2),\n",
    "            encoded_labels,\n",
    "            logits_lens,\n",
    "            labels_len\n",
    "        )\n",
    "        \n",
    "    def val_step(self, images, labels):\n",
    "        logits = self.predict(images)\n",
    "        loss = self.loss_func(logits, labels)\n",
    "        return logits, loss\n",
    "    \n",
    "    def train_step(self, optimizer, images, labels):\n",
    "        logits = self.predict(images)\n",
    "        optimizer.zero_grad()\n",
    "        loss = self.loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return logits, loss\n",
    "    \n",
    "    def predict(self, img):\n",
    "        return self.crnn(img.to(self.device))\n",
    "    \n",
    "    def train(self, num_epochs, optimizer, train_loader, test_loader, print_every=2):\n",
    "        train_losses, valid_losses = [], []\n",
    "        for epoch in range(num_epochs):\n",
    "            total_train_loss = 0\n",
    "            self.crnn.train()\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                logits, train_loss = self.train_step(\n",
    "                    optimizer,\n",
    "                    images,\n",
    "                    labels\n",
    "                )\n",
    "                total_train_loss += train_loss.item()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                total_val_loss = 0\n",
    "                self.crnn.eval()\n",
    "                for i, (images, labels) in enumerate(test_loader):\n",
    "                    logits, val_loss = self.val_step(images, labels)\n",
    "                    total_val_loss += val_loss.item()\n",
    "                train_loss = total_train_loss / len(train_loader.dataset)\n",
    "                val_loss = total_val_loss / len(test_loader.dataset)\n",
    "                train_losses.append(train_loss)\n",
    "                valid_losses.append(val_loss)\n",
    "            if epoch % print_every == 0:\n",
    "                print(f'Epoch {epoch} | train loss {train_loss} | valid loss {val_loss}')\n",
    "        return train_losses, valid_losses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "3e2aa68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | train loss 0.4274300157342992 | valid loss 0.4842488698870222\n",
      "Epoch 2 | train loss 0.4110152246053345 | valid loss 0.45506636673044937\n",
      "Epoch 4 | train loss 0.3541468947599734 | valid loss 0.3445999867448183\n",
      "Epoch 6 | train loss 0.18630591564089338 | valid loss 0.23117989803029\n",
      "Epoch 8 | train loss 0.07404775445582587 | valid loss 0.0740313958898883\n"
     ]
    }
   ],
   "source": [
    "ocr = OCR()\n",
    "optimizer = optim.SGD(\n",
    "            ocr.crnn.parameters(), \n",
    "            lr=0.02, \n",
    "            nesterov=True,\n",
    "            weight_decay=1e-5,\n",
    "            momentum=0.7\n",
    "        )\n",
    "\n",
    "train_losses, val_losses = ocr.train(\n",
    "    EPOCHS,\n",
    "    optimizer, \n",
    "    train_loader,\n",
    "    val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be84b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0766f859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd0cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061dce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd8939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d812c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ee69c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d959ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4190ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd5c452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895c2b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ca8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faac7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9f4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
