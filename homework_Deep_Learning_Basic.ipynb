{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64efce43",
   "metadata": {},
   "source": [
    "### Подготовка данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e090bc",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3ff6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.functional import char_error_rate\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# for training\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58226016",
   "metadata": {},
   "source": [
    "Static configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "883d46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_2_SAMPELS = 'samples/*'\n",
    "TRAIN_DIR = 'data/train'\n",
    "TEST_DIR = 'data/test'\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "ALPHABETH = 'abcdefghijklmnopqrstuvwxyz'\n",
    "DIGITS = '0123456789'\n",
    "CHARS = ALPHABETH + DIGITS\n",
    "VOCAB_SIZE = len(CHARS) + 1\n",
    "\n",
    "EPOCHS = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7627cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    class for getting data and labels\n",
    "    \"\"\"\n",
    "    def __init__(self, pth):\n",
    "        \"\"\"\n",
    "        collect data\n",
    "        \"\"\"\n",
    "        pth_list = os.listdir(pth)\n",
    "        abs_pth = os.path.abspath(pth)\n",
    "        print(abs_pth)\n",
    "        self.imgs = [os.path.join(abs_pth, p) for p in pth_list]\n",
    "        self.transform = transforms.Compose([\n",
    "          transforms.ToTensor()  \n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        get length of the data\n",
    "        \"\"\"\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        get tensor of the image and label by index\n",
    "        \"\"\"\n",
    "        pth = self.imgs[idx]\n",
    "        label = os.path.basename(pth).split('.')[0].lower().strip()\n",
    "        img = Image.open(pth).convert('RGB')\n",
    "        img_tensor = self.transform(img)\n",
    "        return img_tensor, label\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e442603a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb1d31ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aleksandr/Desktop/captcha_recognition/data/train\n",
      "/Users/aleksandr/Desktop/captcha_recognition/data/test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# prepare folders\n",
    "os.system(f'mkdir data')\n",
    "os.system(f\"rm -rf data/train\")\n",
    "os.system(f\"rm -rf data/test\")\n",
    "[os.system(f'mkdir data/{directory}') for directory in ('train', 'test')]\n",
    "\n",
    "\n",
    "# split data for train and test 1/4\n",
    "train_set, test_set = train_test_split(glob.glob(PATH_2_SAMPELS), test_size=0.1)\n",
    "[os.system(f\"cp {te} {TEST_DIR}/{te.split('/')[1]}\") for te in test_set]\n",
    "[os.system(f\"cp {tr} {TRAIN_DIR}/{tr.split('/')[1]}\") for tr in train_set]\n",
    "\n",
    "train_dataset = CaptchDataset(TRAIN_DIR)\n",
    "val_dataset = CaptchDataset(TEST_DIR)\n",
    "\n",
    "\n",
    "# loading data\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680385c",
   "metadata": {},
   "source": [
    "#### Создание и обучение модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674d7ea",
   "metadata": {},
   "source": [
    "CRNN class with layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec5a7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, dropout=0.5):\n",
    "        \"\"\"\n",
    "        initiate layers\n",
    "        \"\"\"\n",
    "        super(CNNRNN, self).__init__()\n",
    "        # probability of an element to be zeroed (Bernoulli distribution)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.clayer = nn.Sequential(\n",
    "            # With square kernels and equal stride and with padding\n",
    "            nn.Conv2d(\n",
    "                in_channels=3, # RGB\n",
    "                out_channels=32, # quatity of kernels\n",
    "                kernel_size=(3,3), # width of kernel\n",
    "                stride=1, # step of kernel\n",
    "                padding=1 # add for input image\n",
    "            ),\n",
    "            # activation max(0,x)\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=(2,2),\n",
    "                stride=2\n",
    "            ),\n",
    "            # in_channels, out_channels, kernel_size\n",
    "            nn.Conv2d(\n",
    "                in_channels=32, # conv in layer t-1 out_channel\n",
    "                out_channels=64,\n",
    "                kernel_size=(3,3),\n",
    "                stride=1, \n",
    "                padding=1),\n",
    "            nn.ReLU(),\n",
    "            # pool of square window of size=2, stride=2\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=(2,2), \n",
    "                stride=2\n",
    "            ),\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=(3,3), \n",
    "                stride=1, \n",
    "                padding=1\n",
    "            ),\n",
    "#             nn.BatchNorm2d(128),\n",
    "            nn.ReLU(), # max(0,x)\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                in_channels=128,\n",
    "                out_channels=256, \n",
    "                kernel_size=(3,3), \n",
    "                stride=1, \n",
    "                padding=1\n",
    "            ),\n",
    "#             nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=(1,2), \n",
    "                stride=2\n",
    "            ),\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                in_channels=256,\n",
    "                out_channels=512,\n",
    "                kernel_size=(3,3), \n",
    "                stride=1, \n",
    "                padding=1,\n",
    "#                 bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                in_channels=512,\n",
    "                out_channels=512,\n",
    "                kernel_size=(3,3), \n",
    "                stride=1, \n",
    "                padding=1,\n",
    "#                 bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=(1, 2), \n",
    "                stride=2),\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                in_channels=512,\n",
    "                out_channels=512,\n",
    "                kernel_size=(2,2), \n",
    "                stride=1, \n",
    "                padding=0\n",
    "            ),\n",
    "            self.dropout\n",
    "        )\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=1024, \n",
    "                out_features=256\n",
    "            ),\n",
    "            self.dropout\n",
    "        )\n",
    "        # RNN to an input sequence\n",
    "        self.rnn_layer = nn.LSTM(\n",
    "            input_size=256,\n",
    "            hidden_size=256, \n",
    "            num_layers=2, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        # Linear transformation to the incoming data\n",
    "        self.outlayer = nn.Linear(\n",
    "            in_features=512, \n",
    "            out_features=vocab_size # quantity of classes\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        method for CNN and RNN combining \n",
    "        \"\"\"\n",
    "        x = self.clayer(x) # CNN apply\n",
    "        x = x.permute(0, 3, 1, 2) # transformate dimension [0, 1, 2, 3]->[0, 3, 1, 2]\n",
    "        x = x.view(x.size(0), x.size(1), -1) # choose field\n",
    "        x = self.seq(x) # apply linear with dropout\n",
    "        x, _ = self.rnn_layer(x) # apply rnn layers\n",
    "        x = self.outlayer(x) # apply linear layer\n",
    "        return x.permute(1, 0, 2) # transformate dimension\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd34f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b13648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCR:\n",
    "    \"\"\"\n",
    "    module for training and validating\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        var inits\n",
    "        \"\"\"\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') # choose device\n",
    "        self.crnn = CNNRNN(VOCAB_SIZE).to(self.device) # init layers\n",
    "        self.critertion = nn.CTCLoss() # The Connectionist Temporal Classification loss. \n",
    "        # Calculates loss between a continuous (unsegmented) time series and a target sequence. \n",
    "        self.char2idx, self.idx2char = self.char_idx()\n",
    "        \n",
    "        \n",
    "    def char_idx(self):\n",
    "        \"\"\"\n",
    "        function get dependence between chars and indexes\n",
    "        \"\"\"\n",
    "        char2idx = {}\n",
    "        idx2char = {}\n",
    "        chars = CHARS.lower() + '-'\n",
    "        for i, chara in enumerate(chars):\n",
    "            char2idx[chara] = i + 1\n",
    "            idx2char[i + 1] = chara\n",
    "        return char2idx, idx2char\n",
    "    \n",
    "    def encode(self, labels):\n",
    "        \"\"\"\n",
    "        function chars to code\n",
    "        \"\"\"\n",
    "        length_per_label = [len(label) for label in labels]\n",
    "        joined_label = ''.join(labels)\n",
    "        joined_encoding = []\n",
    "        for chara in joined_label:\n",
    "            joined_encoding.append(self.char2idx[chara])\n",
    "        return torch.IntTensor(joined_encoding), torch.IntTensor(length_per_label)\n",
    "    \n",
    "    def decode(self, logits):\n",
    "        \"\"\"\n",
    "        function characters to code\n",
    "        \"\"\"\n",
    "        tokens = logits.softmax(2).argmax(2).squeeze(1)\n",
    "        tokens = ''.join([self.idx2char[token]\n",
    "                         if token != 0 else '-'\n",
    "                         for token in tokens.numpy()])\n",
    "        tokens = tokens.split('-')\n",
    "        \n",
    "        text = [chara for batch_token in tokens \\\n",
    "               for idx, chara in enumerate(batch_token) \\\n",
    "               if chara != batch_token[idx-1] or len(batch_token) == 1]\n",
    "        text = ''.join(text)\n",
    "        return text\n",
    "    \n",
    "    def loss_func(self, logits, labels):\n",
    "        encoded_labels, labels_len = self.encode(labels) # tensor of labels in code and tensor of each length of label\n",
    "        logits_lens = torch.full(\n",
    "            size=(logits.size(1),),\n",
    "            fill_value=logits.size(0),\n",
    "            dtype=torch.int32\n",
    "        ).to(self.device)\n",
    "        \n",
    "        return self.critertion(\n",
    "            logits.log_softmax(2),\n",
    "            encoded_labels,\n",
    "            logits_lens,\n",
    "            labels_len\n",
    "        )\n",
    "        \n",
    "    def val_step(self, images, labels):\n",
    "        logits = self.predict(images)\n",
    "        loss = self.loss_func(logits, labels)\n",
    "        return logits, loss\n",
    "    \n",
    "    def train_step(self, optimizer, images, labels):\n",
    "        logits = self.predict(images) # get logits after predict\n",
    "        optimizer.zero_grad() # no calc grads\n",
    "        loss = self.loss_func(logits, labels) # get losses\n",
    "        loss.backward() # backwrd pass\n",
    "#         clip_grad_norm_(self.crnn.parameters(), max_norm=1)\n",
    "        optimizer.step() # graidient move\n",
    "        return logits, loss\n",
    "    \n",
    "    def predict(self, img):\n",
    "        \"\"\"\n",
    "        predict logits by image\n",
    "        \"\"\"\n",
    "        return self.crnn(img.to(self.device))\n",
    "    \n",
    "    def train(self, num_epochs, optimizer, train_loader, test_loader, print_every=2):\n",
    "        train_losses, valid_losses = [], []\n",
    "        for epoch in range(num_epochs):\n",
    "            total_train_loss = 0\n",
    "            self.crnn.train() # switch on train\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                logits, train_loss = self.train_step(\n",
    "                    optimizer,\n",
    "                    images,\n",
    "                    labels\n",
    "                )\n",
    "                total_train_loss += train_loss.item()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                total_val_loss = 0\n",
    "                self.crnn.eval()\n",
    "                for i, (images, labels) in enumerate(test_loader):\n",
    "                    logits, val_loss = self.val_step(images, labels)\n",
    "                    total_val_loss += val_loss.item()\n",
    "                train_loss = total_train_loss / len(train_loader.dataset)\n",
    "                val_loss = total_val_loss / len(test_loader.dataset)\n",
    "                train_losses.append(train_loss)\n",
    "                valid_losses.append(val_loss)\n",
    "            if epoch % print_every == 0:\n",
    "                print(f'Epoch {epoch} | train loss {round(train_loss, 2)} | valid loss {round(val_loss, 2)}')\n",
    "        return train_losses, valid_losses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5ab200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = len(list(np.arange(0.001, 0.04, 0.002)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cc7566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e2aa68c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 13\u001b[0m\n\u001b[1;32m      4\u001b[0m ocr \u001b[38;5;241m=\u001b[39m OCR()\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(\n\u001b[1;32m      6\u001b[0m             ocr\u001b[38;5;241m.\u001b[39mcrnn\u001b[38;5;241m.\u001b[39mparameters(), \n\u001b[1;32m      7\u001b[0m             lr\u001b[38;5;241m=\u001b[39mlr, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m             momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m\n\u001b[1;32m     11\u001b[0m         )\n\u001b[0;32m---> 13\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mocr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m min_loss[lr] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(val_losses)\n\u001b[1;32m     20\u001b[0m count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 106\u001b[0m, in \u001b[0;36mOCR.train\u001b[0;34m(self, num_epochs, optimizer, train_loader, test_loader, print_every)\u001b[0m\n\u001b[1;32m    104\u001b[0m total_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrnn\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_loader):\n\u001b[1;32m    107\u001b[0m     logits, val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_step(images, labels)\n\u001b[1;32m    108\u001b[0m     total_val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m val_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Desktop/dl/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Desktop/dl/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Desktop/dl/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Desktop/dl/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m, in \u001b[0;36mCaptchDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     28\u001b[0m label \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(pth)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     29\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(pth)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img_tensor, label\n",
      "File \u001b[0;32m~/Desktop/dl/venv/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/Desktop/dl/venv/lib/python3.8/site-packages/torchvision/transforms/transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/dl/venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:163\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    162\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 163\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    166\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "File \u001b[0;32m~/Desktop/dl/venv/lib/python3.8/site-packages/PIL/Image.py:701\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 701\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (\u001b[38;5;167;01mMemoryError\u001b[39;00m, \u001b[38;5;167;01mRecursionError\u001b[39;00m)):\n",
      "File \u001b[0;32m~/Desktop/dl/venv/lib/python3.8/site-packages/PIL/Image.py:764\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;66;03m# unpack data\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m e \u001b[38;5;241m=\u001b[39m \u001b[43m_getencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m e\u001b[38;5;241m.\u001b[39msetimage(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim)\n\u001b[1;32m    767\u001b[0m bufsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m65536\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m)  \u001b[38;5;66;03m# see RawEncode.c\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/dl/venv/lib/python3.8/site-packages/PIL/Image.py:422\u001b[0m, in \u001b[0;36m_getencoder\u001b[0;34m(mode, encoder_name, args, extra)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     args \u001b[38;5;241m=\u001b[39m ()\n\u001b[0;32m--> 422\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m(args, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    423\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_loss = {}\n",
    "for lr in list(np.arange(0.001, 0.04, 0.002)):\n",
    "    lr = np.round(lr,3)\n",
    "    ocr = OCR()\n",
    "    optimizer = optim.SGD(\n",
    "                ocr.crnn.parameters(), \n",
    "                lr=lr, \n",
    "                nesterov=True,\n",
    "                weight_decay=1e-5,\n",
    "                momentum=0.7\n",
    "            )\n",
    "\n",
    "    train_losses, val_losses = ocr.train(\n",
    "        EPOCHS,\n",
    "        optimizer, \n",
    "        train_loader,\n",
    "        val_loader\n",
    "    )\n",
    "    min_loss[lr] = min(val_losses)\n",
    "    count -= 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c74d8a",
   "metadata": {},
   "source": [
    "view results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be84b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_show(val_dataset):\n",
    "    samples = []\n",
    "    for i in range(10):\n",
    "        with torch.no_grad():\n",
    "            idx = np.random.randint(len(val_dataset))\n",
    "            img, label = val_dataset[idx]\n",
    "            logits = ocr.predict(img.unsqueeze(0))\n",
    "            pred_text = ocr.decode(logits.cpu())\n",
    "\n",
    "            samples.append((img, label, pred_text))\n",
    "\n",
    "    fig = plt.figure(figsize=(17,5))\n",
    "    for i in range(10):\n",
    "        ax = fig.add_subplot(2, 5, i + 1, xticks=[], yticks=[])\n",
    "\n",
    "        img, label, pred_text = samples[i]\n",
    "        title = f'True: {label} | Pred: {pred_text}'\n",
    "        ax.imshow(img.permute(1,2,0))\n",
    "        ax.set_title(title)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_show(val_dataset)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec987b8f",
   "metadata": {},
   "source": [
    "plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd0cbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(train_losses, val_losses):\n",
    "    plt.plot(train_losses, label='Train_loss')\n",
    "    plt.plot(val_losses, label='Valid_loss')\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_loss(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(df_):\n",
    "    TRUE = []\n",
    "    PRED = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(df_)):\n",
    "            img, label = df_[i]\n",
    "            logits = ocr.predict(img.unsqueeze(0))\n",
    "            pred = ocr.decode(logits.cpu())\n",
    "            TRUE.append(label)\n",
    "            PRED.append(pred)\n",
    "    TRUE = pd.Series(TRUE, name='TRUE')\n",
    "    PRED = pd.Series(PRED, name='PRED')\n",
    "    df_res = pd.concat([TRUE,PRED], axis=1)\n",
    "    return df_res\n",
    "            \n",
    "df_res = get_predictions(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd8939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d812c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(df_res['TRUE'], df_res['PRED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d959ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_error_rate(df_res['TRUE'], df_res['PRED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4190ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd5c452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895c2b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ca8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faac7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9f4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
